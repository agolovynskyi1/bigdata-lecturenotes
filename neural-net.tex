\section{Нейронні мережі}

Типи нейронних мереж за задачею

\begin{itemize}
 \item класифікатор;
 \item генератор; 
 \item TODO.
\end{itemize}


\subsection{Персептрон}

У 1943 році Воррен Маккалох і Волтер Піттс у статті «Логічне числення ідей, що стосуються нервової активності» запропонували поняття штучної нейронної мережі. Зокрема, ними було запропоновано модель штучного нейрону. Дональд Гебб в роботі «Організація поведінки» 1949 року описав основні принципи навчання нейронів. 

Комп'ютерна модель була вперше запропонвана Френком Розенблатом в 1957 році і реалізована у вигляді електронної машини «Марк-1» у 1960 році. Перцептрон став однією з перших моделей нейромереж, а «Марк-1» — першим у світі нейрокомп'ютером. Незважаючи на свою простоту, перцептрон здатен навчатися і розв'язувати досить складні завдання.

\begin{ozn}
Перцептро́н, або персептро́н (англ. perceptron від лат. perceptio — сприйняття; нім. perzeptron) -- це математична функція $f: \R^n \rightarrow \R^m$ або комп'ютерна модель сприйняття інформації мозком, представлена у вигляді
\begin{equation}
 f_j(x) = \sigma (\sum_i x_i w_i - \theta)
\end{equation}
де $x_i$ -- вхідні дані (компоненти тензора $x$),  $w_i$ -- вагові коефіцієнти, $\sigma: \R \rightarrow [0, 1]$ -- нелінійна функція активації, $f_j$ -- це компонента вихідного тензора.
\end{ozn}


Приклад: Cifar 10 \url{https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html}


\subsection{Конволюційні шари}

Приклад. Mnist

Приклад. Модернізація розумного замка: розпізнавання характерного звуку. Якщо немає, клас ``чого прийшов'', якщо є ``заходь, друже''. Для побудови потрібен датасет звуків природи або міських.

Приклад. Визначення треків руху покупця у супермаркеті за відео з камер спостереження. Спрощений приклад: знаходження героїв у мультфільмі. 



\subsubsection{MaxPooling}

Max pooling operation for spatial data.

Input (batchsize, rows, cols, channels) 

Output (batchsize, pooledrows, pooledcols, channels)

\subsubsection{BatchNormalization}

Normalize the activations of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.


\subsection{Автоенкодер}

Формування вкладення (embedding)

https://blog.keras.io/building-autoencoders-in-keras.html

Зниження розмірності датасета.

Приклад. Перетворення текста у картинку.



\subsection{Рекурентні шари}

