\section{Базові методи роботи з даними}

\subsection{Візуалізація даних}

У 3D Python 
% https://jakevdp.github.io/PythonDataScienceHandbook/04.12-three-dimensional-plotting.html

% PCA T-SNE для MNIST: https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b
\subsection{Зниження розмірності векторного простору}

\subsection{Статистичні оцінки}

Нехай X -- набір даних $x_i \in X, i \in \N$, представлений у вигляді тензора.

\begin{ozn}
 Математичне сподівання $M(X) = \frac{1}{n}\sum_{i = 1}^n x_i$. Оскільки операції у тензорах $x_i$ покомпонентні, то результатом є тензор тієї ж розмірності, з елементами $M(x_j)$, де $j \in \N$ пробігає по компонентам тензора.
\end{ozn}


\subsection{Хеммінга}

\begin{ozn}
Нехай $w_1$, $w_2$ два вектори однакової довжини. Відстанню Хеммінга між ними буде кількість компонент, у яких їх значення від
\end{ozn}


% In information theory, the Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In other words, it measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other. In a more general context, the Hamming distance is one of several string metrics for measuring the edit distance between two sequences. It is named after the American mathematician Richard Hamming.

% A major application is in coding theory, more specifically to block codes, in which the equal-length strings are vectors over a finite field. 

\subsection{Метод головних компонент}


Метод головних компонент (англ. principal component analysis) це перетворення координат простору таким чином, що перша головна компонента, у якої найбільша дисперсія, переходить у першу координату, друга у другу і так далі.

Нехай $X$ -- набір даних з $n$ елементів, кожен елемент це вектор з $\R^p$. Він може бути представлений у вигляді матриці, у якої у строках знаходяться елементи $x_i \in X, x_i = (x_{i1}, x_{i2}, \dots, x_{ip})$. Вхідні дані перетворені таким чином, що по кожному ствобчику матриці її середнє,  дорівнює нулю, тобто від кожної компоненти вектора відняли її середнє і $\sum x_i  = 0$.


% Відшукується перша головна компонента як розв'язок задачі:
% 
% \begin{equation}
%  a_1 = argmin ‖ a_1 ‖ = 1 ( \sum i = 1 m ‖ x i − a 1 ( a 1 , x i ) ‖ 2 ) {\displaystyle a_{1}={\underset {\Vert a_{1}\Vert =1}{\operatorname {argmin} }}\left(\sum _{i=1}^{m}\Vert x_{i}-a_{1}(a_{1},x_{i})\Vert ^{2}\right)} {\displaystyle a_{1}={\underset {\Vert a_{1}\Vert =1}{\operatorname {argmin} }}\left(\sum _{i=1}^{m}\Vert x_{i}-a_{1}(a_{1},x_{i})\Vert ^{2}\right)}.
% \end{equation}
% 
% якщо розв'язок не єдиний, то вибирається один з них.
% 
% 
% Mathematically, the transformation is defined by a set of p-dimensional vectors of weights or coefficients w ( k ) = ( w 1 , … , w p ) ( k ) {\displaystyle \mathbf {w} _{(k)}=(w_{1},\dots ,w_{p})_{(k)}} \mathbf {w} _{(k)}=(w_{1},\dots ,w_{p})_{(k)} that map each row vector x ( i ) {\displaystyle \mathbf {x} _{(i)}} \mathbf{x}_{(i)} of X to a new vector of principal component scores t ( i ) = ( t 1 , … , t l ) ( i ) {\displaystyle \mathbf {t} _{(i)}=(t_{1},\dots ,t_{l})_{(i)}} {\displaystyle \mathbf {t} _{(i)}=(t_{1},\dots ,t_{l})_{(i)}}, given by
% 
%     t k ( i ) = x ( i ) ⋅ w ( k ) f o r i = 1 , … , n k = 1 , … , l {\displaystyle {t_{k}}_{(i)}=\mathbf {x} _{(i)}\cdot \mathbf {w} _{(k)}\qquad \mathrm {for} \qquad i=1,\dots ,n\qquad k=1,\dots ,l} {\displaystyle {t_{k}}_{(i)}=\mathbf {x} _{(i)}\cdot \mathbf {w} _{(k)}\qquad \mathrm {for} \qquad i=1,\dots ,n\qquad k=1,\dots ,l}
% 
% in such a way that the individual variables t 1 , … , t l {\displaystyle t_{1},\dots ,t_{l}} {\displaystyle t_{1},\dots ,t_{l}} of t considered over the data set successively inherit the maximum possible variance from X, with each coefficient vector w constrained to be a unit vector (where l {\displaystyle l} l is usually selected to be less than p {\displaystyle p} p to reduce dimensionality). 

Приклади. Модуль PCА у Python

Автоенкодери

\subsection{Визначення залежностей}

Регресія
Логістична регресія

\subsection{Кластеризація даних}

Метод К-середніх

